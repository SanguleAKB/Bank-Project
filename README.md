# BERT (Bidirectional Encoder Representations from Transformers) 
   Encoder: BERT employs an encoder-only architecture.
   Decoder: GPT employs a decoder-only architecture. GPT, or a generative pre-trained transformer, 
              is a type of large language model (LLM) that utilizes deep 
              learning to produce human-like text. Neural networks are 
              trained on massive datasets containing text and code, 
              enabling them to understand and generate coherent and 
              contextually relevant responses. As a key component in the 
              field of generative AI, GPT pushes the boundaries of what's 
              possible with AI, enabling machines to produce creative 
              and human-quality content.
